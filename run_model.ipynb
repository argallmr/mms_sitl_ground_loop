{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMS SITL Ground Loop: Running the GLS MP Model\n",
    "This notebook runs the `mp-dl-unh` GLS Magnetopause model to make predictions. The end result is a CSV file with model selections. This notebook and the model can be run using Level-2 (L2) data, which is publicly available at the [SDC](https://lasp.colorado.edu/mms/sdc/public/), but operationally is ran using SITL- or Quick Look (QL)-level data, which is not public and requires a username and password.\n",
    "\n",
    "<span style=\"font-size:larger;\">**Contents**</span>\n",
    "* [Front Matter](#front_matter)\n",
    "* [Setting Up](#setting_up)\n",
    "* [Download Data](#download_data)\n",
    "  * [FGM](#download_fgm)\n",
    "  * [EDP](#download_edp)\n",
    "  * [DIS](#download_dis)\n",
    "  * [DES](#download_des)\n",
    "  * [Combine Dataframes](#combine_dataframes)\n",
    "* [Run the Model](#run_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='front_matter'></a>\n",
    "## Front Matter\n",
    "This section defines the packages to import, data and model directories, SDC log-in credentials, and model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pymms\n",
    "from pymms.util.tai import utc2tai\n",
    "from pymms.sdc import mrmms_sdc_api as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.constants\n",
    "from cdflib import cdfread, epochs\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import sklearn # Required to open scaler.sav file\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Disables Tensorflow debugging information\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data_root` is the directory in which all data files are downloaded. It, as well as your MMS log-in credentials, can be set more permanently in the configuration file within pymms. The `dropbox_root` is where the new model predictions generated by this notebook will be saved, and `model_root` is just the mms-sitl-ground-loop package directory where this notebook and the model parameters are kept. Here, we assume that `model_root` is in the current working directory, which [may not be true](https://github.com/ipython/ipython/issues/10123)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-In Credentials\n",
    "#   - these can be set in pymms/config.ini\n",
    "#   - you may need to set the model_root\n",
    "# pymms.config['data_root'] = ''\n",
    "# pymms.config['username'] = ''\n",
    "# pymms.config['password'] = ''\n",
    "model_root = os.getcwd()\n",
    "if not os.path.isfile(os.path.join(model_root, 'model.model')):\n",
    "    raise ValueError('Could not automatically determine the model root.')\n",
    "\n",
    "# Paths\n",
    "data_path_root = Path(pymms.config['data_root']).expanduser().absolute().resolve()\n",
    "dropbox_root = data_path_root / 'mp_dl_unh_predictions/'\n",
    "model_root = Path(model_root).expanduser().absolute().resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the spacecraft, data rate mode, data level, and interval in which to make predictions. For the model to perform accurately, `level='sitl'`; however, the SITL-level data is not publicly available and requires an SDC username and password. If you do not have a password for the SDC, you can change to `level='l2'` to use the public database.\n",
    "\n",
    "The interval can be specified either by orbit number or by `datetime.datetime` object. Normally, predictions are made using the start time of the first sub-region of interest (SROI) and the end time of the last SROI. If the interval is an orbit number, the SROI time intervals are obtained automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from orbit 1067 SROI1\n",
    "sc = 'mms1'\n",
    "level = 'sitl' # 'sitl' (private) or 'l2' (public)\n",
    "start_interval = 1067\n",
    "end_interval = 1067\n",
    "# start_interval = datetime.datetime(2019, 12, 17, 15, 45, 59)\n",
    "# start_interval = datetime.datetime(2019, 12, 18, 5, 29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the user inputs and define static inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITL data is available in the fast-survey region of the orbit.\n",
    "# For many instruments, fast- and slow-survey data are combined into a single survey product\n",
    "mode = 'srvy'\n",
    "\n",
    "# This script works only for 'sitl' and 'l2' data\n",
    "if level not in ('sitl', 'l2'):\n",
    "    raise ValueError('Level must be either \"sitl\" or \"l2\".')\n",
    "\n",
    "# If an orbit number is given, \n",
    "if isinstance(start_interval, int):\n",
    "    sroi = api.mission_events('sroi', start_interval, start_interval, sc=sc)\n",
    "    start_date = sroi['tstart'][0]\n",
    "    end_date = sroi['tend'][-1]\n",
    "else:\n",
    "    start_date = start_interval\n",
    "    end_date = end_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setting_up'></a>\n",
    "## Setting Up\n",
    "Use the [SDC API](https://lasp.colorado.edu/mms/sdc/public/about/how-to/) to search for available files within the desired time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interface to the SDC\n",
    "mms = api.MrMMS_SDC_API(sc=sc, mode=mode, start_date=start_date, end_date=end_date)\n",
    "\n",
    "# Ensure that the log-in information is there.\n",
    "#   - If the config file was already set, this step is redundant.\n",
    "mms._data_root = pymms.config['data_root']\n",
    "if mode == 'sitl':\n",
    "    mms._session.auth(pymms.config['username'], pymms.config['password'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='download_data'></a>\n",
    "## Download and Read Data\n",
    "Start downloading data. We use two helper functions: `read_cdf_vars` to read variable data from CDF files and `quality_factor` to compute a burst trigger quality values as in Section 3.3 of [Phan et al. 2015](http://dx.doi.org/10.1007/s11214-015-0150-2).\n",
    "\n",
    "The data used here is from the [AFG](http://dx.doi.org/10.1007/s11214-014-0057-3), EDP (from [SDP](http://dx.doi.org/10.1007/s11214-014-0116-9) and [ADP](http://dx.doi.org/10.1007/s11214-014-0115-x)), [DIS, and DES](http://dx.doi.org/10.1007/s11214-016-0245-4) instruments. CDF files are download, data is read and stored as Pandas data frames. Additional metafeatures are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cdf_vars(cdf_files, cdf_vars, epoch='Epoch'):\n",
    "    '''\n",
    "    Read variables from CDF files into a data frame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cdf_files : str or list\n",
    "        CDF files to be read\n",
    "    cdf_vars : str or list\n",
    "        Names of the variables to be read\n",
    "    epoch : str\n",
    "        Name of the time variable that serves as the data frame index\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : `pandas.DataFrame`\n",
    "        The data. If a variable is 2D, \"_#\" is appended, where \"#\"\n",
    "        increases from 0 to var.shape[1]-1.\n",
    "    '''\n",
    "    tepoch = epochs.CDFepoch()\n",
    "    if isinstance(cdf_files, str):\n",
    "        cdf_files = [cdf_files]\n",
    "    if isinstance(cdf_vars, str):\n",
    "        cdf_vars = [cdf_vars]\n",
    "    if epoch not in cdf_vars:\n",
    "        cdf_vars.append(epoch)\n",
    "    \n",
    "    out = []\n",
    "    for file in cdf_files:\n",
    "        file_df = pd.DataFrame()\n",
    "        cdf = cdfread.CDF(file)\n",
    "        \n",
    "        for var_name in cdf_vars:\n",
    "            # Read the variable data\n",
    "            data = cdf.varget(var_name)\n",
    "            if var_name == epoch:\n",
    "                data = tepoch.to_datetime(data, to_np=True)\n",
    "            \n",
    "            # Store as column in data frame\n",
    "            if data.ndim == 1:\n",
    "                file_df[var_name] = data\n",
    "                \n",
    "            # 2D variables get \"_#\" appended to name for each column\n",
    "            elif data.ndim == 2:\n",
    "                for idx in range(data.shape[1]):\n",
    "                    file_df['{0}_{1}'.format(var_name, idx)] = data[:,idx]\n",
    "                    \n",
    "            # 3D variables gets reshaped to 2D and treated as 2D\n",
    "            # This includes variables like the pressure and temperature tensors\n",
    "            elif data.ndim == 3:\n",
    "                dims = data.shape\n",
    "                data = data.reshape(dims[0], dims[1]*dims[2])\n",
    "                for idx in range(data.shape[1]):\n",
    "                    file_df['{0}_{1}'.format(var_name, idx)] = data[:,idx]\n",
    "            else:\n",
    "                print('cdf_var.ndims > 3. Skipping. {0}'.format(var_name))\n",
    "                continue\n",
    "        \n",
    "        # Close the file\n",
    "        cdf.close()\n",
    "        \n",
    "        # Set the epoch variable as the index\n",
    "        file_df.set_index(epoch, inplace=True)\n",
    "        out.append(file_df)\n",
    "    \n",
    "    # Concatenate all of the file data\n",
    "    out = pd.concat(out)\n",
    "    \n",
    "    # Check that the index is unique\n",
    "    # Some contiguous low-level data files have data overlap at the edges of the files (e.g., AFG)\n",
    "    if not out.index.is_unique:\n",
    "        out['index'] = out.index\n",
    "        out.drop_duplicates(subset='index', inplace=True, keep='first')\n",
    "        out.drop(columns='index', inplace=True)\n",
    "    \n",
    "    # File names are not always given in order, so sort the data\n",
    "    out.sort_index(inplace=True)\n",
    "    return out      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_df_cols(df, old_col, new_cols):\n",
    "    '''\n",
    "    Each column of a multi-dimensional CDF variable gets stored as\n",
    "    its own independent column in the DataFrame, with \"_#\" appended\n",
    "    to the original variable name to indicate which column index\n",
    "    the column was taken from. This function renames those columns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : `pandas.DataFrame`\n",
    "        DataFrame for which the columns are to be renamed\n",
    "    old_col : str\n",
    "        Name of the column (sans \"_#\")\n",
    "    new_cols : list\n",
    "        New names to be given to the columns\n",
    "    '''\n",
    "    df.rename(columns={'{}_{}'.format(old_col, idx): new_col_name\n",
    "                       for idx, new_col_name in enumerate(new_cols)},\n",
    "              inplace=True\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_factor(data, M=2):\n",
    "    '''\n",
    "    Compute a quality factor for burst triggers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : `numpy.ndarray`\n",
    "        One dimensional data array\n",
    "    M : int\n",
    "        Smoothing factor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Q : `numpy.ndarray`\n",
    "        Burst trigger quality factor\n",
    "    '''\n",
    "    smoothed_data = [data[0]]\n",
    "    for i, value in enumerate(data[1:]):\n",
    "        smoothed_data.append((smoothed_data[i - 1] * (2 ** M - 1) + value) / 2 ** M)\n",
    "    return np.subtract(data, smoothed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fgm_data'></a>\n",
    "### FGM Data\n",
    "Download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two magnetometers: AFG and DFG. For L2 data, AFG is\n",
    "# used for slow survey and DFG is used for fast survey, but are\n",
    "# known by the instrument name FGM. For SITL-level data, the\n",
    "# instruments are separate and named as AFG and DFG.\n",
    "afg_instr = 'afg'\n",
    "if level == 'l2':\n",
    "    afg_instr = 'fgm'\n",
    "\n",
    "afg_mode = mode\n",
    "\n",
    "# The \"SITL\"-level data for AFG is labeled \"ql\" for quick-look\n",
    "afg_level = level\n",
    "if level == 'sitl':\n",
    "    afg_level = 'ql'\n",
    "\n",
    "afg_optdesc = None\n",
    "\n",
    "# Download the data files\n",
    "mms.instr = afg_instr\n",
    "mms.mode = afg_mode\n",
    "mms.level = afg_level\n",
    "mms.optdesc = afg_optdesc\n",
    "afg_files = mms.download_files()\n",
    "print(*afg_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the variable names from a sample file\n",
    "afg_cdf = cdfread.CDF(afg_files[0])\n",
    "info = afg_cdf.cdf_info()\n",
    "afg_cdf.close()\n",
    "print(*info['zVariables'], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable names\n",
    "t_vname = 'Epoch'\n",
    "if afg_level == 'l2':\n",
    "    b_vname = '_'.join((sc, afg_instr, 'b', 'dmpa', afg_mode, afg_level))\n",
    "else:\n",
    "    b_vname = '_'.join((sc, afg_instr, afg_mode, 'dmpa'))\n",
    "\n",
    "# Read the data\n",
    "afg_df = read_cdf_vars(afg_files, b_vname, epoch=t_vname)\n",
    "\n",
    "# Rename variables\n",
    "rename_df_cols(afg_df, b_vname, ('Bx', 'By', 'Bz', '|B|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metafeatures and store data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metafeatures\n",
    "afg_df['P_B'] = afg_df['|B|']**2 / scipy.constants.mu_0\n",
    "afg_df['clock_angle'] = np.arctan2(afg_df['By'], afg_df['Bz'])\n",
    "afg_df['Q_dBx'] = quality_factor(afg_df['Bx'])\n",
    "afg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='edp_data'></a>\n",
    "### EDP Data\n",
    "Download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edp_instr = 'edp'\n",
    "edp_optdesc = 'dce'\n",
    "\n",
    "# EDP does not have \"srvy\" data, just \"fast\" and \"slow\"\n",
    "edp_mode = mode\n",
    "if mode == 'srvy':\n",
    "    edp_mode = 'fast'\n",
    "\n",
    "# The \"SITL\"-level data for EDP is labeled \"ql\" for quick-look\n",
    "edp_level = level\n",
    "if level == 'sitl':\n",
    "    edp_level = 'ql'\n",
    "\n",
    "# Download the data files\n",
    "mms.instr = edp_instr\n",
    "mms.mode = edp_mode\n",
    "mms.optdesc = edp_optdesc\n",
    "edp_files = mms.download_files()\n",
    "print(*edp_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the variable names from a sample file\n",
    "edp_cdf = cdfread.CDF(edp_files[0])\n",
    "info = edp_cdf.cdf_info()\n",
    "edp_cdf.close()\n",
    "print(*info['zVariables'], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable names\n",
    "if level == 'l2':\n",
    "    t_vname = '_'.join((sc, edp_instr, 'epoch', edp_mode, edp_level))\n",
    "    e_vname = '_'.join((sc, edp_instr, edp_optdesc, 'dsl', edp_mode, edp_level))\n",
    "else:\n",
    "    t_vname = '_'.join((sc, edp_instr, edp_optdesc, 'epoch'))\n",
    "    e_vname = '_'.join((sc, edp_instr, edp_optdesc, 'xyz', 'dsl'))\n",
    "\n",
    "# Read the data\n",
    "edp_df = read_cdf_vars(edp_files, e_vname, epoch=t_vname)\n",
    "\n",
    "# Rename variables\n",
    "new_vnames = ('Ex', 'Ey', 'Ez')\n",
    "edp_df.rename(columns={'{}_{}'.format(e_vname, idx): vname\n",
    "                       for idx, vname in enumerate(new_vnames)},\n",
    "              inplace=True\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metafeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edp_df['|E|'] = np.sqrt(edp_df['Ex']**2 + edp_df['Ey']**2 + edp_df['Ez']**2)\n",
    "edp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dis_data'></a>\n",
    "### DIS Data\n",
    "Download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_instr = 'fpi'\n",
    "\n",
    "# FPI does not have \"srvy\" data, just \"fast\" and \"slow\"\n",
    "dis_mode = mode\n",
    "if mode == 'srvy':\n",
    "    dis_mode = 'fast'\n",
    "\n",
    "# The \"SITL\"-level data for FPI is labeled \"ql\" for quick-look\n",
    "# There is SITL-level data, but it was discontinued early in the mission\n",
    "dis_level = level\n",
    "if level == 'sitl':\n",
    "    dis_level = 'ql'\n",
    "\n",
    "dis_optdesc = 'dis'\n",
    "if level == 'l2':\n",
    "    dis_optdesc = 'dis-moms'\n",
    "\n",
    "# Download the data files\n",
    "mms.instr = dis_instr\n",
    "mms.mode = dis_mode\n",
    "mms.level = dis_level\n",
    "mms.optdesc = dis_optdesc\n",
    "dis_files = mms.download_files()\n",
    "print(*dis_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the variable names from a sample file\n",
    "dis_cdf = cdfread.CDF(dis_files[0])\n",
    "info = dis_cdf.cdf_info()\n",
    "print(*info['zVariables'], sep='\\n')\n",
    "\n",
    "# Print information about the pressure tensor\n",
    "# to figure out its dimensions and how the components\n",
    "# are stored\n",
    "vname = '_'.join((sc, 'dis', 'prestensor', 'dbcs', dis_mode))\n",
    "var_notes = dis_cdf.attget(attribute='VAR_NOTES', entry=vname)\n",
    "print(var_notes['Data'])\n",
    "\n",
    "# Close the file\n",
    "dis_cdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable names\n",
    "t_vname = 'Epoch'\n",
    "espectr_omni_vname = '_'.join((sc, 'dis', 'energyspectr', 'omni', dis_mode))\n",
    "n_vname = '_'.join((sc, 'dis', 'numberdensity', dis_mode))\n",
    "v_vname = '_'.join((sc, 'dis', 'bulkv', 'dbcs', dis_mode))\n",
    "q_heat_vname = '_'.join((sc, 'dis', 'heatq', 'dbcs', dis_mode))\n",
    "t_para_vname = '_'.join((sc, 'dis', 'temppara', dis_mode))\n",
    "t_perp_vname = '_'.join((sc, 'dis', 'tempperp', dis_mode))\n",
    "t_tens_vname = '_'.join((sc, 'dis', 'temptensor', 'dbcs', dis_mode))\n",
    "p_tens_vname = '_'.join((sc, 'dis', 'prestensor', 'dbcs', dis_mode))\n",
    "\n",
    "# Read the data\n",
    "dis_df = read_cdf_vars(dis_files,\n",
    "                       [espectr_omni_vname, n_vname, v_vname,\n",
    "                        q_heat_vname, t_para_vname, t_perp_vname,\n",
    "                        p_tens_vname, t_tens_vname\n",
    "                       ],\n",
    "                       epoch=t_vname\n",
    "                      )\n",
    "\n",
    "# Rename variables\n",
    "dis_df.rename(columns={n_vname: 'Ni'}, inplace=True)\n",
    "dis_df.rename(columns={t_para_vname: 'Ti_para'}, inplace=True)\n",
    "dis_df.rename(columns={t_perp_vname: 'Ti_perp'}, inplace=True)\n",
    "rename_df_cols(dis_df, v_vname, ('Vix', 'Viy', 'Viz'))\n",
    "rename_df_cols(dis_df, q_heat_vname, ('Qi_xx', 'Qi_yy', 'Qi_zz'))\n",
    "rename_df_cols(dis_df, t_tens_vname,\n",
    "               ('Ti_xx', 'Ti_xy', 'Ti_xz', 'Ti_yx', 'Ti_yy', 'Ti_yz', 'Ti_zx', 'Ti_zy', 'Ti_zz'))\n",
    "rename_df_cols(dis_df, p_tens_vname,\n",
    "               ('Pi_xx', 'Pi_xy', 'Pi_xz', 'Pi_yx', 'Pi_yy', 'Pi_yz', 'Pi_zx', 'Pi_zy', 'Pi_zz'))\n",
    "rename_df_cols(dis_df, espectr_omni_vname, ['especi_{0}'.format(idx) for idx in range(32)])\n",
    "\n",
    "# Drop redundant components of the pressure and temperature tensors\n",
    "dis_df.drop(columns=['Ti_xy', 'Ti_xz', 'Ti_yz', 'Pi_xy', 'Pi_xz', 'Pi_yz'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metafeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dis_df['Ti_anisotropy'] = (dis_df['Ti_para'] / dis_df['Ti_perp']) - 1\n",
    "dis_df['Ti_scalar'] = (dis_df['Ti_para'] + 2*dis_df['Ti_perp']) / 3.0\n",
    "dis_df['Q_dNi'] = quality_factor(dis_df['Ni'])\n",
    "dis_df['Q_dViz'] = quality_factor(dis_df['Viz'])\n",
    "Vi_mag = np.sqrt(dis_df['Vix']**2 + dis_df['Viy']**2 + dis_df['Viz']**2)\n",
    "Pi_ram = dis_df['Ni'] * Vi_mag\n",
    "dis_df['Q_dPi_ram'] = quality_factor(Pi_ram)\n",
    "\n",
    "# Drop features that were accidentally excluded\n",
    "dis_df.drop(columns=['especi_31', 'Viz', 'Qi_zz'], inplace=True)\n",
    "\n",
    "print(dis_df.columns)\n",
    "dis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='des_data'></a>\n",
    "### DES Data\n",
    "Download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_instr = 'fpi'\n",
    "\n",
    "# FPI does not have \"srvy\" data, just \"fast\" and \"slow\"\n",
    "des_mode = mode\n",
    "if mode == 'srvy':\n",
    "    des_mode = 'fast'\n",
    "\n",
    "# The \"SITL\"-level data for FPI is labeled \"ql\" for quick-look\n",
    "# There is SITL-level data, but it was discontinued early in the mission\n",
    "des_level = level\n",
    "if level == 'sitl':\n",
    "    des_level = 'ql'\n",
    "\n",
    "des_optdesc = 'des'\n",
    "if level == 'l2':\n",
    "    des_optdesc = 'des-moms'\n",
    "\n",
    "# Download the data files\n",
    "mms.instr = des_instr\n",
    "mms.mode = des_mode\n",
    "mms.level = des_level\n",
    "mms.optdesc = des_optdesc\n",
    "des_files = mms.download_files()\n",
    "print(*des_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the variable names from a sample file\n",
    "des_cdf = cdfread.CDF(des_files[0])\n",
    "info = des_cdf.cdf_info()\n",
    "print(*info['zVariables'], sep='\\n')\n",
    "\n",
    "# Print information about the pressure tensor\n",
    "# to figure out its dimensions and how the components\n",
    "# are stored\n",
    "vname = 'mms1_des_prestensor_dbcs_fast'\n",
    "var_notes = des_cdf.attget(attribute='VAR_NOTES', entry=vname)\n",
    "print(var_notes['Data'])\n",
    "\n",
    "# Close the file\n",
    "des_cdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable names\n",
    "t_vname = 'Epoch'\n",
    "espectr_omni_vname = '_'.join((sc, 'des', 'energyspectr', 'omni', des_mode))\n",
    "n_vname = '_'.join((sc, 'des', 'numberdensity', des_mode))\n",
    "v_vname = '_'.join((sc, 'des', 'bulkv', 'dbcs', des_mode))\n",
    "q_heat_vname = '_'.join((sc, 'des', 'heatq', 'dbcs', des_mode))\n",
    "t_para_vname = '_'.join((sc, 'des', 'temppara', des_mode))\n",
    "t_perp_vname = '_'.join((sc, 'des', 'tempperp', des_mode))\n",
    "t_tens_vname = '_'.join((sc, 'des', 'temptensor', 'dbcs', des_mode))\n",
    "p_tens_vname = '_'.join((sc, 'des', 'prestensor', 'dbcs', des_mode))\n",
    "\n",
    "# Read the data\n",
    "des_df = read_cdf_vars(des_files,\n",
    "                       [espectr_omni_vname, n_vname, v_vname,\n",
    "                        q_heat_vname, t_para_vname, t_perp_vname,\n",
    "                        p_tens_vname, t_tens_vname\n",
    "                       ],\n",
    "                       epoch=t_vname\n",
    "                      )\n",
    "\n",
    "# Rename variables\n",
    "des_df.rename(columns={n_vname: 'Ne'}, inplace=True)\n",
    "des_df.rename(columns={t_para_vname: 'Te_para'}, inplace=True)\n",
    "des_df.rename(columns={t_perp_vname: 'Te_perp'}, inplace=True)\n",
    "rename_df_cols(des_df, v_vname, ('Vex', 'Vey', 'Vez'))\n",
    "rename_df_cols(des_df, q_heat_vname, ('Qe_xx', 'Qe_yy', 'Qe_zz'))\n",
    "rename_df_cols(des_df, t_tens_vname,\n",
    "               ('Te_xx', 'Te_xy', 'Te_xz', 'Te_yx', 'Te_yy', 'Te_yz', 'Te_zx', 'Te_zy', 'Te_zz'))\n",
    "rename_df_cols(des_df, p_tens_vname,\n",
    "               ('Pe_xx', 'Pe_xy', 'Pe_xz', 'Pe_yx', 'Pe_yy', 'Pe_yz', 'Pe_zx', 'Pe_zy', 'Pe_zz'))\n",
    "rename_df_cols(des_df, espectr_omni_vname, ['espece_{0}'.format(idx) for idx in range(32)])\n",
    "\n",
    "# Drop symmetric, redundant components\n",
    "des_df.drop(columns=['Te_xy', 'Te_xz', 'Te_yz', 'Pe_xy', 'Pe_xz', 'Pe_yz'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metafeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "des_df['Te_anisotropy'] = (des_df['Te_para'] / des_df['Te_perp']) - 1\n",
    "des_df['Te_scalar'] = (des_df['Te_para'] + 2*des_df['Te_perp']) / 3.0\n",
    "des_df['Pe_scalar'] = (des_df['Pe_xx'] + des_df['Pe_yy'] + des_df['Pe_zz']) / 3.0\n",
    "des_df['Q_dNe'] = quality_factor(des_df['Ne'])\n",
    "des_df['Q_dVez'] = quality_factor(des_df['Vez'])\n",
    "#Ve_mag = np.sqrt(des_df['Vex']**2 + des_df['Vey']**2 + des_df['Vez']**2)\n",
    "#Pe_ram = des_df['Ne'] * Ve_mag\n",
    "#des_df['Q_dPe_ram'] = quality_factor(Pe_ram)\n",
    "\n",
    "\n",
    "# Drop features that were accidentally excluded\n",
    "des_df.drop(columns=['espece_31', 'Vez', 'Qe_zz'], inplace=True)\n",
    "\n",
    "print(des_df.columns)\n",
    "des_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='combine_dataframes'></a>\n",
    "### Combine DataFrames and Compute Additional Metafeatures\n",
    "Now we will combine all dataframes, downsampling to DES, which as the time index with the longest sampling period (`4.5s`). After that, multi-instrument metafeatures are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Resample data\n",
    "afg_df = afg_df.reindex(des_df.index, method='nearest')\n",
    "edp_df = edp_df.reindex(des_df.index, method='nearest')\n",
    "dis_df = dis_df.reindex(des_df.index, method='nearest')\n",
    "\n",
    "# Merge dataframes\n",
    "df = des_df\n",
    "df = df.join(dis_df, how='outer')\n",
    "df = df.join(afg_df, how='outer')\n",
    "df = df.join(edp_df, how='outer')\n",
    "\n",
    "# Metafeatures\n",
    "df['T_ratio'] = df['Ti_scalar'] / df['Ti_scalar']\n",
    "df['plasma_beta'] = (df['Ti_scalar'] + df['Ti_scalar']) / df['|E|']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*df.columns, sep=', ')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run_model'></a>\n",
    "## Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundTime(dt=None, dateDelta=datetime.timedelta(minutes=1)):\n",
    "    \"\"\"Round a datetime object to a multiple of a timedelta\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dt : `datetime.datetime` = datetime.datetime.now().\n",
    "    dateDelta : `datetime.timedelta`\n",
    "        we round to a multiple of this, default 1 minute.\n",
    "    \n",
    "    Author: Thierry Husson 2012 - Use it as you want but don't blame me.\n",
    "            Stijn Nevens 2014 - Changed to use only datetime objects as variables\n",
    "    \"\"\"\n",
    "    roundTo = dateDelta.total_seconds()\n",
    "\n",
    "    if dt == None : dt = datetime.datetime.now()\n",
    "    seconds = (dt - dt.min).seconds\n",
    "    # // is a floor division, not a comment on following line:\n",
    "    rounding = (seconds+roundTo/2) // roundTo * roundTo\n",
    "    return dt + datetime.timedelta(0,rounding-seconds,-dt.microsecond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(num_features=123, layer_size=300):\n",
    "    \"\"\"\n",
    "    Helper function to define the LSTM used to make predictions.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Bidirectional(LSTM(layer_size, return_sequences=True, activation='tanh', recurrent_activation='sigmoid'),\n",
    "                            input_shape=(None, num_features)))\n",
    "\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(layer_size, return_sequences=True, activation='tanh', recurrent_activation='sigmoid')))\n",
    "\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define MMS CDF directory location\n",
    "# Load model\n",
    "model = lstm()\n",
    "model.load_weights(str(model_root / 'model_weights.h5'))\n",
    "\n",
    "# Interpolate interior values, drop outside rows containing 0s\n",
    "data = df.replace([np.inf, -np.inf], np.nan)\n",
    "data = data.interpolate(method='time', limit_area='inside')\n",
    "data = data.loc[(df != 0).any(axis=1)]\n",
    "\n",
    "# Select data within time range\n",
    "data = data.loc[start_date:end_date]\n",
    "data_index = data.index\n",
    "\n",
    "# Scale data\n",
    "scaler = pickle.load(open(model_root / 'scaler.sav', 'rb'))\n",
    "data = scaler.transform(data)\n",
    "\n",
    "# Run data through model\n",
    "predictions_list = model.predict(np.expand_dims(data, axis=0))\n",
    "\n",
    "# Filter predictions with threshold\n",
    "threshold = 0.5\n",
    "filtered_output = [0 if x < threshold else 1 for x in predictions_list.squeeze()]\n",
    "\n",
    "# Create selections from predictions\n",
    "predictions_df = pd.DataFrame()\n",
    "predictions_df.insert(0, \"time\", data_index)\n",
    "predictions_df.insert(1, \"prediction\", filtered_output)\n",
    "predictions_df['group'] = (predictions_df.prediction != predictions_df.prediction.shift()).cumsum()\n",
    "predictions_df = predictions_df.loc[predictions_df['prediction'] == 1]\n",
    "selections = pd.DataFrame({'BeginDate': predictions_df.groupby('group').time.first().map(lambda x: roundTime(utc2tai(x), datetime.timedelta(seconds=10))),\n",
    "                           'EndDate': predictions_df.groupby('group').time.last().map(lambda x: roundTime(utc2tai(x), datetime.timedelta(seconds=10)))})\n",
    "selections = selections.set_index('BeginDate')\n",
    "\n",
    "selections['score'] = \"150.0\" # This is a placeholder for the FOM\n",
    "selections['description'] = \"MP crossing (automatically generated)\"\n",
    "\n",
    "# Create the file name: gls_selections_<model-name>_<current-time-as-YYYY-MM-DD-HH-MM-SS>.csv\n",
    "current_datetime = datetime.datetime.now()\n",
    "selections_filetime = current_datetime.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "file_name = 'gls_selections_mp-dl-unh_{0}.csv'.format(selections_filetime)\n",
    "\n",
    "# Output selections\n",
    "print('Saving selections to CSV: {0}'.format(file_name))\n",
    "if not dropbox_root.exists():\n",
    "    dropbox_root.mkdir(parents=True)\n",
    "selections.to_csv(dropbox_root / file_name, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
